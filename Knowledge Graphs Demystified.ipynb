{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Knowledge Graphs Demystified Part 2: Practical Session\n",
    "\n",
    "In this notebook we are going to construct a simple knowledge graph using Python, and run some queries on the graph in Neo4j.\n",
    "\n",
    "If you would like to run this code yourself, you will need to install the `py2neo` package in Python 3.\n",
    "\n",
    "To run part 3 onwards, you will need to install Neo4j, which can be downloaded at https://neo4j.com/download/.\n",
    "\n",
    "I will be running through the code during part 2 of the master class so there is no need to install anything unless you would also like to try the code out yourself and run some graph queries.\n",
    "\n",
    "\n",
    "## 1. Read in the data\n",
    "\n",
    "Before we can build a graph, we must first read in the example datasets:\n",
    "\n",
    "- `work_order_file`: A csv file containing a set of work orders.\n",
    "- `downtime_file`: a csv file containing a set of downtime events.\n",
    "\n",
    "Here is an example of what the first few rows of each dataset look like:\n",
    "\n",
    "![alt text](images/example-data.png \"Example datasets\")\n",
    "\n",
    "\n",
    "\n",
    "We are using the simple `csv` library to read in the data, though this can also be done using `pandas`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('StartDate', '10/07/2005'), ('FLOC', '1234.1.1'), ('ShortText', 'repair cracked hyd tank')])\n",
      "OrderedDict([('StartDate', '14/07/2005'), ('FLOC', '1234.1.2'), ('ShortText', 'engine wont start')])\n",
      "OrderedDict([('StartDate', '17/07/2005'), ('FLOC', '1234.1.3'), ('ShortText', 'a/c blowing hot air')])\n",
      "OrderedDict([('StartDate', '20/07/2005'), ('FLOC', '1234.1.2'), ('ShortText', 'engin u/s')])\n",
      "OrderedDict([('StartDate', '21/07/2005'), ('FLOC', '1234.1.2'), ('ShortText', 'fix engine')])\n",
      "OrderedDict([('StartDate', '22/07/2005'), ('FLOC', '1234.1.4'), ('ShortText', 'pump service')])\n",
      "OrderedDict([('StartDate', '23/07/2005'), ('FLOC', '1234.1.4'), ('ShortText', 'pump leak')])\n",
      "OrderedDict([('StartDate', '24/07/2005'), ('FLOC', '1234.1.4'), ('ShortText', 'fix leak on pump')])\n",
      "OrderedDict([('StartDate', '25/07/2005'), ('FLOC', '1234.1.2'), ('ShortText', 'engine not running')])\n",
      "OrderedDict([('StartDate', '26/07/2005'), ('FLOC', '1234.1.2'), ('ShortText', 'engine has problems starting')])\n",
      "OrderedDict([('StartDate', '27/07/2005'), ('FLOC', '1234.1.4'), ('ShortText', 'pump fault')])\n",
      "OrderedDict([('StartDate', '28/07/2005'), ('FLOC', '1234.1.4'), ('ShortText', 'pump leaking')])\n",
      "OrderedDict([('StartDate', '29/07/2005'), ('FLOC', '1234.1.3'), ('ShortText', 'a/c not working')])\n",
      "OrderedDict([('StartDate', '30/07/2005'), ('FLOC', '1234.1.3'), ('ShortText', 'a/c broken')])\n"
     ]
    }
   ],
   "source": [
    "from csv import DictReader\n",
    "\n",
    "work_order_file = \"data/sample_work_orders.csv\"\n",
    "downtime_file = \"data/sample_downtime_events.csv\"\n",
    "\n",
    "# A simple function to read in a csv file and return a list,\n",
    "# where each element in the list is a dictionary of {heading : value}\n",
    "def load_csv(filename):\n",
    "    data = []\n",
    "    with open(filename, 'r') as f:\n",
    "        reader = DictReader(f)\n",
    "        for row in reader:\n",
    "            data.append(row)\n",
    "    return data\n",
    "\n",
    "        \n",
    "work_order_data = load_csv(work_order_file)\n",
    "downtime_data = load_csv(downtime_file)\n",
    "\n",
    "for row in work_order_data:\n",
    "    print(row)\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Construct nodes from the entities in the short text\n",
    "\n",
    "Our first task is to extract the entities in the short text descriptions and construct nodes from those entities. This is how we are able to unlock the knowledge captured within the short text and combine it with the structured fields.\n",
    "\n",
    "![alt text](images/extracting-entities-v2.png \"Extracting entities\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Define a Lexicon Tagger class\n",
    "\n",
    "Extracting the entities in the short text is typically done using a trained Named Entity Recognition model, however for simplicity we will use a Lexicon.\n",
    "\n",
    "The LexiconTagger class is a simple alternative to a trained named entity recognition model such as an LSTM or Transformer. \n",
    "\n",
    "This class serves to automatically extract entities from each sentence using a predefined lexicon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the lexicon\n",
    "\n",
    "import itertools\n",
    "\n",
    "class LexiconTagger:\n",
    "    \"\"\" A lexicon-based entity tagger.\n",
    "    \n",
    "    Args:\n",
    "        lexicon_file: The filename of the lexicon.\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, lexicon_file, max_ngram_size = 3):\n",
    "        \n",
    "        lexicon_data = load_csv(lexicon_file)\n",
    "        self.max_ngram_size = max_ngram_size\n",
    "        \n",
    "        # Convert the loaded csv into a dictionary mapping word(s) to entity class\n",
    "        self.lexicon = {}\n",
    "        for row in lexicon_data:\n",
    "            self.lexicon[row[\"key\"]] = row[\"value\"]      \n",
    "        \n",
    "        \n",
    "    \n",
    "    def get_ngrams(self, sentence):        \n",
    "        \"\"\"\n",
    "            Given a sentence, return a list of all combinations of ngrams up to a certain size.\n",
    "            \n",
    "            Args:\n",
    "                sentence: A list of words, e.g. [\"fix\", \"broken\", \"pump\"].\n",
    "                \n",
    "            Returns:\n",
    "                ngrams: A list of ngrams containing up to max_ngram_size words.\n",
    "                        For example, given the input [\"fix\", \"broken\", \"pump\"],\n",
    "                        return [\"fix\", \"broken\", \"pump\", \"fix broken\", \"broken pump\", \"fix broken pump\"] \n",
    "        \n",
    "        \"\"\"\n",
    "        ngrams = []        \n",
    "        for n in range(self.max_ngram_size):\n",
    "            for c in itertools.combinations(sentence, n + 1):\n",
    "                ngrams.append(\" \".join(c))\n",
    "        return ngrams\n",
    "    \n",
    "    def extract_entities(self, sentence):   \n",
    "        \"\"\"\n",
    "            Given a sentence (a list of words), return a list of (word, entity_class) pairs.\n",
    "            \n",
    "            Args:\n",
    "                sentence: A list of words.\n",
    "            \n",
    "            Returns:\n",
    "                ngram_entity_pairs: A list of tuples, where each tuple contains (ngram, entity_class), for example\n",
    "                                    (\"not working\", \"observation\").\n",
    "        \n",
    "        \"\"\"\n",
    "        ngram_entity_pairs = []        \n",
    "        for ngram in self.get_ngrams(sentence):\n",
    "            if ngram in self.lexicon:\n",
    "                entity_class = self.lexicon[ngram]\n",
    "                ngram_entity_pairs.append( (ngram, entity_class))\n",
    "        return ngram_entity_pairs\n",
    "    \n",
    "    def normalise_ngram(self, ngram):\n",
    "        \"\"\" \n",
    "            Given an ngram, return the equivalent item in the lexicon.\n",
    "            \n",
    "            Args:\n",
    "                ngram: The ngram to normalise.\n",
    "            \n",
    "            Returns:\n",
    "                normalised_ngram: The normalised version of the ngram according to the lexicon.\n",
    "                                  If the ngram is not present in the lexicon, return the ngram itself.\n",
    "        \"\"\"\n",
    "        normalised_ngram = self.lexicon[ngram] if ngram in self.lexicon else ngram\n",
    "        return normalised_ngram\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Extract entities from each work order\n",
    "\n",
    "Now that we have defined a lexicon tagger, we can process each row in the work order data and extract a set of (ngram, entity_class) pairs for each row.\n",
    "\n",
    "An \"ngram\" is a group of one or more words, for example \"pump\", \"hydraulic tank\", \"blowing hot air\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('repair', 'activity'), ('cracked', 'observation'), ('hyd tank', 'item')]\n",
      "[('engine', 'item'), ('wont start', 'observation')]\n",
      "[('a/c', 'item'), ('blowing hot air', 'observation')]\n",
      "[('engin', 'item')]\n",
      "[('fix', 'activity'), ('engine', 'item')]\n",
      "[('pump', 'item'), ('service', 'activity')]\n",
      "[('pump', 'item'), ('leak', 'observation')]\n",
      "[('fix', 'activity'), ('leak', 'observation'), ('pump', 'item')]\n",
      "[('engine', 'item'), ('not running', 'observation')]\n",
      "[('engine', 'item'), ('problems starting', 'observation')]\n",
      "[('pump', 'item'), ('fault', 'observation')]\n",
      "[('pump', 'item'), ('leaking', 'observation')]\n",
      "[('a/c', 'item'), ('not working', 'observation')]\n",
      "[('a/c', 'item'), ('broken', 'observation')]\n"
     ]
    }
   ],
   "source": [
    "lexicon_file = \"data/lexicon.csv\"\n",
    "lexicon_tagger = LexiconTagger(lexicon_file)\n",
    "\n",
    "work_order_entities = []\n",
    "\n",
    "for row in work_order_data:\n",
    "    sentence = row[\"ShortText\"].split() # We must 'tokenise' the sentence first, i.e. split into words.\n",
    "    ngram_entity_pairs = lexicon_tagger.extract_entities(sentence)\n",
    "    work_order_entities.append(ngram_entity_pairs)\n",
    "    \n",
    "for row in work_order_entities:\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Normalise the entities\n",
    "\n",
    "The next step is to normalise the ngrams, i.e. convert each ngram into a normalised form. This is important as we would prefer to have a single node for a single concept, e.g. one node for \"engine\" as opposed to two nodes for \"engin\" and \"engine\".\n",
    "\n",
    "We will once again be using a lexicon for this task, but it would typically be performed by machine learning.\n",
    "\n",
    "![alt text](images/normalising-entities.png \"Normalising entities\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('repair', 'activity'), ('cracked', 'observation'), ('hydraulic tank', 'item')]\n",
      "[('engine', 'item'), ('failure to start', 'observation')]\n",
      "[('air conditioner', 'item'), ('overheating', 'observation')]\n",
      "[('engine', 'item')]\n",
      "[('fix', 'activity'), ('engine', 'item')]\n",
      "[('pump', 'item'), ('service', 'activity')]\n",
      "[('pump', 'item'), ('leak', 'observation')]\n",
      "[('fix', 'activity'), ('leak', 'observation'), ('pump', 'item')]\n",
      "[('engine', 'item'), ('breakdown', 'observation')]\n",
      "[('engine', 'item'), ('failure to start', 'observation')]\n",
      "[('pump', 'item'), ('electrical issue', 'observation')]\n",
      "[('pump', 'item'), ('leak', 'observation')]\n",
      "[('air conditioner', 'item'), ('breakdown', 'observation')]\n",
      "[('air conditioner', 'item'), ('breakdown', 'observation')]\n"
     ]
    }
   ],
   "source": [
    "lexicon_n_file = \"data/lexicon_normalisation.csv\"\n",
    "lexicon_normaliser = LexiconTagger(lexicon_n_file)\n",
    "\n",
    "normalised_work_order_entities = []\n",
    "\n",
    "# For every row in work_order_entities, replace each ngram with its normalised counterpart\n",
    "# as per the normalisation lexicon.\n",
    "# For example, \"engin\" will become \"engine\", \"leaking\" will become \"leak\", etc.\n",
    "for row in work_order_entities:\n",
    "    normalised_work_order_entities.append([(lexicon_normaliser.normalise_ngram(ngram), entity_class) \n",
    "                                           for (ngram, entity_class) in row])\n",
    "    \n",
    "    \n",
    "for row in normalised_work_order_entities:\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Extract relations between the entities\n",
    "\n",
    "Now that we have our normalised set of (ngram, entity_class) pairs for each work order, we need to build the relationships between them.\n",
    "\n",
    "In our graph we are going to link each \"item\" to every other entity appearing in the work order.\n",
    "\n",
    "![alt text](images/building-relations.png \"Building relations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(('hydraulic tank', 'item'), 'HAS_ACTIVITY', ('repair', 'activity'))\n",
      "(('hydraulic tank', 'item'), 'HAS_OBSERVATION', ('cracked', 'observation'))\n",
      "(('engine', 'item'), 'HAS_OBSERVATION', ('failure to start', 'observation'))\n",
      "(('air conditioner', 'item'), 'HAS_OBSERVATION', ('overheating', 'observation'))\n",
      "(('engine', 'item'), 'HAS_ACTIVITY', ('fix', 'activity'))\n",
      "(('pump', 'item'), 'HAS_ACTIVITY', ('service', 'activity'))\n",
      "(('pump', 'item'), 'HAS_OBSERVATION', ('leak', 'observation'))\n",
      "(('pump', 'item'), 'HAS_ACTIVITY', ('fix', 'activity'))\n",
      "(('pump', 'item'), 'HAS_OBSERVATION', ('leak', 'observation'))\n",
      "(('engine', 'item'), 'HAS_OBSERVATION', ('breakdown', 'observation'))\n",
      "(('engine', 'item'), 'HAS_OBSERVATION', ('failure to start', 'observation'))\n",
      "(('pump', 'item'), 'HAS_OBSERVATION', ('electrical issue', 'observation'))\n",
      "(('pump', 'item'), 'HAS_OBSERVATION', ('leak', 'observation'))\n",
      "(('air conditioner', 'item'), 'HAS_OBSERVATION', ('breakdown', 'observation'))\n",
      "(('air conditioner', 'item'), 'HAS_OBSERVATION', ('breakdown', 'observation'))\n"
     ]
    }
   ],
   "source": [
    "triples = []\n",
    "\n",
    "for row in normalised_work_order_entities:\n",
    "    for (ngram, entity_class) in row:\n",
    "        if entity_class != \"item\": continue\n",
    "            \n",
    "        # If this entity is an item, link it to all other entities in the work order       \n",
    "             \n",
    "        for (other_ngram, other_entity_class) in row:   \n",
    "            if ngram == other_ngram: continue # Don't link items to themselves                \n",
    "\n",
    "            relation_type = other_entity_class.upper()                \n",
    "            triples.append(((ngram, entity_class), \"HAS_%s\" % relation_type, (other_ngram, other_entity_class)))\n",
    "        \n",
    "for triple in triples:\n",
    "    print(triple)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create the graph\n",
    "\n",
    "Now that we have our nodes and relations we can go ahead and build the Neo4J graph.\n",
    "\n",
    "To do this we are going to use py2neo, a Python library for interacting with Neo4J.\n",
    "\n",
    "There are also a couple of other ways to do this - you can either use Neo4J and run Cypher queries to insert each node and relation, or use the APOC library to import a list of nodes from a CSV file. I find Python to be the simplest way, however.\n",
    "\n",
    "> Before proceeding, make sure you have created a new graph in Neo4j and that your new Neo4j graph is running.\n",
    "\n",
    "You can download and install Neo4j from here if you haven't already: https://neo4j.com/download/. I will be demonstrating the graph during the class so there's no need to have it installed unless you are also interested in trying out some graph queries yourself.\n",
    "\n",
    "#### If you need to build your graph again, make sure to run this cell before running subsequent cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from py2neo import Graph\n",
    "from py2neo.data import Node, Relationship\n",
    "\n",
    "GRAPH_PASSWORD = \"password\" # Set this to the password of your Neo4J graph\n",
    "\n",
    "graph = Graph(password = GRAPH_PASSWORD)\n",
    "\n",
    "# We will start by deleting all nodes and edges in the current graph.\n",
    "# If we don't do this, we will end up with duplicate nodes and edges when running this script again.\n",
    "graph.delete_all() \n",
    "\n",
    "tx = graph.begin()\n",
    "\n",
    "# We will keep a dictionary of nodes that we have created so far.\n",
    "# This serves two purposes:\n",
    "#  - prevents duplicate nodes\n",
    "#  - provides us with a way to create edges between the nodes\n",
    "created_entity_nodes = {}\n",
    "\n",
    "# Creates a node for the specified ngram and entity_class.\n",
    "# If the node has already been created (i.e. it exists in created_nodes), return the node.\n",
    "# Otherwise, create a new one.\n",
    "def create_entity_node(ngram, entity_class):\n",
    "    if ngram in created_entity_nodes:\n",
    "        node = created_entity_nodes[ngram]\n",
    "    else:\n",
    "        node = Node(\"Entity\", entity_class, name=ngram)\n",
    "        created_entity_nodes[ngram] = node\n",
    "        tx.create(node)\n",
    "    return node\n",
    "\n",
    "\n",
    "# Create a node for each triple in the list of triples.\n",
    "# Set the class of each node to the entity_class (e.g. \"activity\", \"item\" or \"observation\").\n",
    "# Create a relationship between the nodes in the triple.\n",
    "for ((ngram_1, entity_class_1), relation, (ngram_2, entity_class_2)) in triples:\n",
    "    \n",
    "    node_1 = create_entity_node(ngram_1, entity_class_1)\n",
    "    node_2 = create_entity_node(ngram_2, entity_class_2)   \n",
    "    \n",
    "    \n",
    "    # Create a relationship between two nodes.\n",
    "    # This does not check for duplicate relationships unlike create_node,\n",
    "    # so this code will need to be adjusted on larger datasets.\n",
    "    relationship = Relationship( node_1, relation, node_2 )\n",
    "    tx.create(relationship)\n",
    "    \n",
    "    \n",
    "tx.commit()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Create nodes for the work orders\n",
    "\n",
    "In order to query our graph, we need to create nodes for each work order in our dataset as well. We then need to link each Document node to every Entity node appearing in that document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dateutil.parser import parse as parse_date\n",
    "\n",
    "# Our work_order_data and normalised_work_order entities allow us to do this quite easily,\n",
    "\n",
    "tx = graph.begin()\n",
    "\n",
    "# We will once again keep a mapping of created work order nodes, this time indexed by the row index.\n",
    "created_work_order_nodes = {}\n",
    "\n",
    "# Dates are a little awkward in Neo4j - we have to convert it to an integer representation in Python.\n",
    "# The APOC library has functions to handle this better.\n",
    "def date_to_int(date):\n",
    "    parsed_date = parse_date(str(date))\n",
    "    date = int(\"%s%s%s\" % (parsed_date.year, str(parsed_date.month).zfill(2), str(parsed_date.day).zfill(2)))\n",
    "    return date\n",
    "\n",
    "# The process of creating a work order node is a bit different to creating an entity,\n",
    "# as we also want to incorporate some of the structured fields onto the node.\n",
    "def create_structured_node(index, row, node_type, created_nodes):\n",
    "    if index in created_nodes:\n",
    "        return created_nodes[index]\n",
    "\n",
    "    if 'StartDate' in row:\n",
    "        row['StartDate'] = date_to_int(row['StartDate'])\n",
    "    if 'EndDate' in row:\n",
    "        row['EndDate'] = date_to_int(row['EndDate'])  \n",
    "\n",
    "    node = Node(node_type, **row)\n",
    "    created_nodes[index] = node\n",
    "    tx.create(node)\n",
    "    return node\n",
    "\n",
    "for i, row in enumerate(work_order_data):\n",
    "    node = create_structured_node(i, row, \"WorkOrder\", created_work_order_nodes)\n",
    "    \n",
    "tx.commit()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Link the entities to their corresponding work order nodes\n",
    "\n",
    "In order to properly query our graph, we need to link every entity node to the work order node in which it appears.\n",
    "\n",
    "This allows us to run queries such as \"pumps with electrical issues in the last 3 months\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "tx = graph.begin()\n",
    "\n",
    "# We can use the normalised_work_order_entries list to do this.\n",
    "for i, row in enumerate(normalised_work_order_entities):\n",
    "    for (ngram, entity_class) in row:        \n",
    "        \n",
    "        node_1 = created_entity_nodes[ngram]\n",
    "        node_2 = created_work_order_nodes[i]\n",
    "        \n",
    "        relationship = Relationship( node_1, \"APPEARS_IN\", node_2 )\n",
    "        tx.create(relationship)\n",
    "       \n",
    "tx.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Extending the graph to incorporate Downtime events\n",
    "\n",
    "The next step is to incorporate the downtime events.\n",
    "\n",
    "For this exercise we are going to link the Downtime events to the first Item node appearing in the work orders with the same FLOC as the downtime event.\n",
    "\n",
    "\n",
    "![alt text](images/adding-downtime-events.png \"Adding downtime events\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "tx = graph.begin()\n",
    "\n",
    "created_downtime_nodes = {}\n",
    "\n",
    "# Create a DowntimeEvent node for each row\n",
    "for i, downtime_row in enumerate(downtime_data):\n",
    "    node = create_structured_node(i, downtime_row, \"DowntimeEvent\", created_downtime_nodes)\n",
    "    \n",
    "    # Get all work order nodes with the same FLOC and link the DowntimeEvent to the Items appearing\n",
    "    # in those work orders\n",
    "    for j, work_order_row in enumerate(work_order_data):\n",
    "        if work_order_row[\"FLOC\"] == downtime_row[\"FLOC\"]:\n",
    "            \n",
    "            work_order_entities = normalised_work_order_entities[j]\n",
    "            \n",
    "            for (ngram, entity_class) in work_order_entities:\n",
    "                if entity_class != \"item\": continue    # We don't need to link non-items to downtime events               \n",
    "                    \n",
    "                item_node = created_entity_nodes[ngram]\n",
    "                relationship = Relationship( item_node, \"HAS_EVENT\", node )\n",
    "                tx.create(relationship)\n",
    "                break\n",
    "\n",
    "    \n",
    "tx.commit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Querying the graph\n",
    "\n",
    "Now that the graph has been created, we can query it in Neo4j. This section lists some example queries that we can run on our graph. If you would like to try these yourself you can paste them directly into the Neo4j console.\n",
    "\n",
    "First, let's try a simple query. Here is a query that searches for __all failure modes observed on engines__:\n",
    "\n",
    "    MATCH (e:Entity {name: \"engine\"})-[r:HAS_OBSERVATION]->(o:observation)\n",
    "    RETURN e, r, o\n",
    "\n",
    "We can also use our graph as a way to quickly search and access work orders for the entities appearing in those work orders. For example, searching for __all work orders containing a leak__:\n",
    "\n",
    "    MATCH (d:WorkOrder)<-[a:APPEARS_IN]-(o:observation {name: \"leak\"})\n",
    "    RETURN d, a, o\n",
    "\n",
    "We could extend this to also show the items on which the leaks were present:\n",
    "\n",
    "    MATCH (d:WorkOrder)<-[a:APPEARS_IN]-(o:observation {name: \"leak\"})<-[r:HAS_OBSERVATION]-(e:Entity)\n",
    "    RETURN d, a, o, r, e\n",
    "\n",
    "Our queries can also incorporate structured data, such as the start dates of the work orders. Here is an example query for __all assets that had leaks from 25 to 28 July__:\n",
    "\n",
    "    MATCH (d:WorkOrder)<-[a:APPEARS_IN]-(e:Entity)-[r:HAS_OBSERVATION]->(o:observation {name: \"leak\"})-[:APPEARS_IN]->(d)\n",
    "    WHERE d.StartDate >= 20050725\n",
    "    AND d.StartDate <= 20050728\n",
    "    RETURN e, r, o\n",
    "\n",
    "On a larger graph this would also work well with other forms of structured data such as costs. We could query based on specific asset costs, for example.\n",
    "\n",
    "Now that our work orders and downtime events are in one graph, we can also make queries about downtime events. Here is an example query for the __downtime events associated with assets appearing in work orders from 25 to 28 July (where the downtime events occurred in July)__:\n",
    "\n",
    "    MATCH (d:WorkOrder)<-[a:APPEARS_IN]-(e:Entity)-[r:HAS_EVENT]->(x:DowntimeEvent)\n",
    "    WHERE d.StartDate > 20050725\n",
    "    AND d.StartDate < 20050728\n",
    "    AND 20050700 <= x.StartDate <= 20050731\n",
    "    RETURN e, r, x\n",
    "\n",
    "We can of course extend this to specific assets, such as pumps:\n",
    "\n",
    "    MATCH (d:WorkOrder)<-[a:APPEARS_IN]-(e:Entity {name: \"pump\"})-[r:HAS_EVENT]->(x:DowntimeEvent)\n",
    "    WHERE d.StartDate > 20050725\n",
    "    AND d.StartDate < 20050728\n",
    "    AND 20050700 <= x.StartDate <= 20050731\n",
    "    RETURN e, r, x\n",
    "\n",
    "In larger graphs the downtime events could even be further queried based on duration, cost, lost feed, or date ranges."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Future improvements\n",
    "\n",
    "### Incorporating FLOCs\n",
    "\n",
    "Our downtime events are currently linked to Item nodes, but it would make more sense to link them to nodes representing the functional locations.\n",
    "\n",
    "If you are interested in continuing work on this small graph, the next best step would be to create nodes for the functional location data (`floc_data`) and to link the downtime events to those nodes as opposed to the Item nodes.\n",
    "\n",
    "![alt text](images/adding-flocs.png \"Adding FLOCs\")\n",
    "\n",
    "### Frequencies on edge properties\n",
    "\n",
    "We could also improve the graph by incorporating frequencies onto the edge properties. For example, if a \"leak\" occurred on a pump in two different work orders, our link between \"pump\" and \"leak\" could have a property called `frequency` with a value of `2`. This would allow us to query, for example, assets that had a particularly high number of leaks.\n",
    "\n",
    "\n",
    "### Constructing a graph from your own work order data\n",
    "\n",
    "If you have a work order dataset of your own, feel free to download this code and try it out on your dataset.\n",
    "\n",
    "If you need to extract entities not listed in the lexicon, you will need to update the lexicon file to include your new entities. Alternatively, the LexiconTagger can be substituted for a named entity recognition model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "floc_file = \"data/sample_flocs.csv\"\n",
    "floc_data = load_csv(floc_file)\n",
    "\n",
    "# Your code here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
