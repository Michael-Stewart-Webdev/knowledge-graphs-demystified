{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Read in the data\n",
    "\n",
    "Before we can build a graph, we must first read in three datasets:\n",
    "- `work_order_file`: A csv file containing a set of work orders.\n",
    "- `floc_file`: a csv file containing a set of FLOCs and their FLOC descriptions.\n",
    "- `downtime_file`: a csv file containing a set of downtime events.\n",
    "\n",
    "We are using the simple `csv` library for this task, though this can also be done using `pandas`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('StartDate', '10/07/2005'), ('FLOC', '1234.1.1'), ('ShortText', 'repair cracked hyd tank')])\n",
      "OrderedDict([('StartDate', '14/07/2005'), ('FLOC', '1234.1.2'), ('ShortText', 'engine wont start')])\n",
      "OrderedDict([('StartDate', '17/07/2005'), ('FLOC', '1234.1.3'), ('ShortText', 'a/c blowing hot air')])\n",
      "OrderedDict([('StartDate', '20/07/2005'), ('FLOC', '1234.1.2'), ('ShortText', 'engin unserviceable')])\n",
      "OrderedDict([('StartDate', '21/07/2005'), ('FLOC', '1234.1.2'), ('ShortText', 'fix engine')])\n",
      "OrderedDict([('StartDate', '22/07/2005'), ('FLOC', '1234.1.4'), ('ShortText', 'pump service')])\n",
      "OrderedDict([('StartDate', '23/07/2005'), ('FLOC', '1234.1.4'), ('ShortText', 'pump leak')])\n",
      "OrderedDict([('StartDate', '24/07/2005'), ('FLOC', '1234.1.4'), ('ShortText', 'fix leak on pump')])\n",
      "OrderedDict([('StartDate', '25/07/2005'), ('FLOC', '1234.1.2'), ('ShortText', 'engine not running')])\n",
      "OrderedDict([('StartDate', '26/07/2005'), ('FLOC', '1234.1.2'), ('ShortText', 'engine has problems starting')])\n",
      "OrderedDict([('StartDate', '27/07/2005'), ('FLOC', '1234.1.4'), ('ShortText', 'pump fault')])\n",
      "OrderedDict([('StartDate', '28/07/2005'), ('FLOC', '1234.1.4'), ('ShortText', 'pump leaking')])\n",
      "OrderedDict([('StartDate', '29/07/2005'), ('FLOC', '1234.1.3'), ('ShortText', 'a/c not working')])\n",
      "OrderedDict([('StartDate', '30/07/2005'), ('FLOC', '1234.1.3'), ('ShortText', 'a/c broken')])\n"
     ]
    }
   ],
   "source": [
    "from csv import DictReader\n",
    "\n",
    "work_order_file = \"data/sample_work_orders.csv\"\n",
    "floc_file = \"data/sample_flocs.csv\"\n",
    "downtime_file = \"data/sample_downtime_events.csv\"\n",
    "\n",
    "# A simple function to read in a csv file and return a list,\n",
    "# where each element in the list is a dictionary of {heading : value}\n",
    "def load_csv(filename):\n",
    "    data = []\n",
    "    with open(filename, 'r') as f:\n",
    "        reader = DictReader(f)\n",
    "        for row in reader:\n",
    "            data.append(row)\n",
    "    return data\n",
    "\n",
    "        \n",
    "work_order_data = load_csv(work_order_file)\n",
    "floc_data = load_csv(floc_file)\n",
    "downtime_data = load_csv(downtime_file)\n",
    "\n",
    "for row in work_order_data:\n",
    "    print(row)\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Construct nodes from the entities in the short text\n",
    "\n",
    "Our first task is to extract the entities in the short text descriptions and construct nodes from those entities. This is how we are able to unlock the knowledge captured within the short text and combine it with the structured fields.\n",
    "\n",
    "![alt text](images/extracting-entities.png \"Extracting entities\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Define a Lexicon Tagger class\n",
    "\n",
    "Extracting the entities in the short text is typically done using a trained Named Entity Recognition model, however for simplicity we will use a Lexicon.\n",
    "\n",
    "The LexiconTagger class is a simple alternative to a trained named entity recognition model such as an LSTM or Transformer. \n",
    "\n",
    "This class serves to automatically extract entities from each sentence using a predefined lexicon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the lexicon\n",
    "\n",
    "import itertools\n",
    "\n",
    "class LexiconTagger:\n",
    "    \"\"\" A lexicon-based entity tagger.\n",
    "    \n",
    "    Args:\n",
    "        lexicon_file: The filename of the lexicon.\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, lexicon_file, max_ngram_size = 3):\n",
    "        \n",
    "        lexicon_data = load_csv(lexicon_file)\n",
    "        self.max_ngram_size = max_ngram_size\n",
    "        \n",
    "        # Convert the loaded csv into a dictionary mapping word(s) to entity class\n",
    "        self.lexicon = {}\n",
    "        for row in lexicon_data:\n",
    "            self.lexicon[row[\"key\"]] = row[\"value\"]      \n",
    "        \n",
    "        \n",
    "    \n",
    "    def get_ngrams(self, sentence):        \n",
    "        \"\"\"\n",
    "            Given a sentence, return a list of all combinations of ngrams up to a certain size.\n",
    "            \n",
    "            Args:\n",
    "                sentence: A list of words, e.g. [\"fix\", \"broken\", \"pump\"].\n",
    "                \n",
    "            Returns:\n",
    "                ngrams: A list of ngrams containing up to max_ngram_size words.\n",
    "                        For example, given the input [\"fix\", \"broken\", \"pump\"],\n",
    "                        return [\"fix\", \"broken\", \"pump\", \"fix broken\", \"broken pump\", \"fix broken pump\"] \n",
    "        \n",
    "        \"\"\"\n",
    "        ngrams = []        \n",
    "        for n in range(self.max_ngram_size):\n",
    "            for c in itertools.combinations(sentence, n + 1):\n",
    "                ngrams.append(\" \".join(c))\n",
    "        return ngrams\n",
    "    \n",
    "    def extract_entities(self, sentence):   \n",
    "        \"\"\"\n",
    "            Given a sentence (a list of words), return a list of (word, entity_class) pairs.\n",
    "            \n",
    "            Args:\n",
    "                sentence: A list of words.\n",
    "            \n",
    "            Returns:\n",
    "                ngram_entity_pairs: A list of tuples, where each tuple contains (ngram, entity_class), for example\n",
    "                                    (\"not working\", \"observation\").\n",
    "        \n",
    "        \"\"\"\n",
    "        ngram_entity_pairs = []        \n",
    "        for ngram in self.get_ngrams(sentence):\n",
    "            if ngram in self.lexicon:\n",
    "                entity_class = self.lexicon[ngram]\n",
    "                ngram_entity_pairs.append( (ngram, entity_class))\n",
    "        return ngram_entity_pairs\n",
    "    \n",
    "    def normalise_ngram(self, ngram):\n",
    "        \"\"\" \n",
    "            Given an ngram, return the equivalent item in the lexicon.\n",
    "            \n",
    "            Args:\n",
    "                ngram: The ngram to normalise.\n",
    "            \n",
    "            Returns:\n",
    "                normalised_ngram: The normalised version of the ngram according to the lexicon.\n",
    "                                  If the ngram is not present in the ngram, return the ngram itself.\n",
    "        \"\"\"\n",
    "        normalised_ngram = self.lexicon[ngram] if ngram in self.lexicon else ngram\n",
    "        return normalised_ngram\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Extract entities from each work order\n",
    "\n",
    "Now that we have defined a lexicon tagger, we can process each row in the work order data and extract a set of (ngram, entity_class) pairs for each row.\n",
    "\n",
    "An \"ngram\" is a group of one or more words, for example \"pump\", \"hydraulic tank\", \"blowing hot air\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('repair', 'activity'), ('cracked', 'observation'), ('hyd tank', 'item')]\n",
      "[('engine', 'item'), ('wont start', 'observation')]\n",
      "[('a/c', 'item'), ('blowing hot air', 'observation')]\n",
      "[('engin', 'item'), ('unserviceable', 'observation')]\n",
      "[('fix', 'activity'), ('engine', 'item')]\n",
      "[('pump', 'item'), ('service', 'activity')]\n",
      "[('pump', 'item'), ('leak', 'observation')]\n",
      "[('fix', 'activity'), ('leak', 'observation'), ('pump', 'item')]\n",
      "[('engine', 'item'), ('not running', 'observation')]\n",
      "[('engine', 'item'), ('problems starting', 'observation')]\n",
      "[('pump', 'item'), ('fault', 'observation')]\n",
      "[('pump', 'item'), ('leaking', 'observation')]\n",
      "[('a/c', 'item'), ('not working', 'observation')]\n",
      "[('a/c', 'item'), ('broken', 'observation')]\n"
     ]
    }
   ],
   "source": [
    "lexicon_file = \"data/lexicon.csv\"\n",
    "lexicon_tagger = LexiconTagger(lexicon_file)\n",
    "\n",
    "work_order_entities = []\n",
    "\n",
    "for row in work_order_data:\n",
    "    sentence = row[\"ShortText\"].split() # We must 'tokenise' the sentence first, i.e. split into words.\n",
    "    ngram_entity_pairs = lexicon_tagger.extract_entities(sentence)\n",
    "    work_order_entities.append(ngram_entity_pairs)\n",
    "    \n",
    "for row in work_order_entities:\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Normalise the ngrams\n",
    "\n",
    "The next step is to normalise the ngrams, i.e. convert each ngram into a normalised form. This is important as we would prefer to have a single node for a single concept, e.g. one node for \"engine\" as opposed to two nodes for \"engin\" and \"engine\".\n",
    "\n",
    "We will once again be using a lexicon for this task, but it would typically be performed by machine learning.\n",
    "\n",
    "![alt text](images/normalising-entities.png \"Normalising entities\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('repair', 'activity'), ('cracked', 'observation'), ('hydraulic tank', 'item')]\n",
      "[('engine', 'item'), ('wont start', 'observation')]\n",
      "[('air conditioner', 'item'), ('overheating', 'observation')]\n",
      "[('engine', 'item'), ('unserviceable', 'observation')]\n",
      "[('fix', 'activity'), ('engine', 'item')]\n",
      "[('pump', 'item'), ('service', 'activity')]\n",
      "[('pump', 'item'), ('leak', 'observation')]\n",
      "[('fix', 'activity'), ('leak', 'observation'), ('pump', 'item')]\n",
      "[('engine', 'item'), ('breakdown', 'observation')]\n",
      "[('engine', 'item'), ('failure to start', 'observation')]\n",
      "[('pump', 'item'), ('electrical issue', 'observation')]\n",
      "[('pump', 'item'), ('leak', 'observation')]\n",
      "[('air conditioner', 'item'), ('breakdown', 'observation')]\n",
      "[('air conditioner', 'item'), ('breakdown', 'observation')]\n"
     ]
    }
   ],
   "source": [
    "lexicon_n_file = \"data/lexicon_normalisation.csv\"\n",
    "lexicon_normaliser = LexiconTagger(lexicon_n_file)\n",
    "\n",
    "normalised_work_order_entities = []\n",
    "\n",
    "# For every row in work_order_entities, replace each ngram with its normalised counterpart\n",
    "# as per the normalisation lexicon.\n",
    "# For example, \"engin\" will become \"engine\", \"leaking\" will become \"leak\", etc.\n",
    "for row in work_order_entities:\n",
    "    normalised_work_order_entities.append([(lexicon_normaliser.normalise_ngram(ngram), entity_class) \n",
    "                                           for (ngram, entity_class) in row])\n",
    "    \n",
    "    \n",
    "for row in normalised_work_order_entities:\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Extract relations between the entities\n",
    "\n",
    "Now that we have our normalised set of (ngram, entity_class) pairs for each work order, we need to build the relationships between them.\n",
    "\n",
    "In our graph we are going to link each \"item\" to every other entity appearing in the work order.\n",
    "\n",
    "![alt text](images/building-relations.png \"Building relations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(('hydraulic tank', 'item'), 'HAS_ACTIVITY', ('repair', 'activity'))\n",
      "(('hydraulic tank', 'item'), 'HAS_OBSERVATION', ('cracked', 'observation'))\n",
      "(('engine', 'item'), 'HAS_OBSERVATION', ('wont start', 'observation'))\n",
      "(('air conditioner', 'item'), 'HAS_OBSERVATION', ('overheating', 'observation'))\n",
      "(('engine', 'item'), 'HAS_OBSERVATION', ('unserviceable', 'observation'))\n",
      "(('engine', 'item'), 'HAS_ACTIVITY', ('fix', 'activity'))\n",
      "(('pump', 'item'), 'HAS_ACTIVITY', ('service', 'activity'))\n",
      "(('pump', 'item'), 'HAS_OBSERVATION', ('leak', 'observation'))\n",
      "(('pump', 'item'), 'HAS_ACTIVITY', ('fix', 'activity'))\n",
      "(('pump', 'item'), 'HAS_OBSERVATION', ('leak', 'observation'))\n",
      "(('engine', 'item'), 'HAS_OBSERVATION', ('breakdown', 'observation'))\n",
      "(('engine', 'item'), 'HAS_OBSERVATION', ('failure to start', 'observation'))\n",
      "(('pump', 'item'), 'HAS_OBSERVATION', ('electrical issue', 'observation'))\n",
      "(('pump', 'item'), 'HAS_OBSERVATION', ('leak', 'observation'))\n",
      "(('air conditioner', 'item'), 'HAS_OBSERVATION', ('breakdown', 'observation'))\n",
      "(('air conditioner', 'item'), 'HAS_OBSERVATION', ('breakdown', 'observation'))\n"
     ]
    }
   ],
   "source": [
    "triples = []\n",
    "\n",
    "for row in normalised_work_order_entities:\n",
    "    for (ngram, entity_class) in row:\n",
    "        if entity_class != \"item\": continue\n",
    "            \n",
    "        # If this entity is an item, link it to all other entities in the work order       \n",
    "             \n",
    "        for (other_ngram, other_entity_class) in row:   \n",
    "            if ngram == other_ngram: continue # Don't link items to themselves                \n",
    "\n",
    "            relation_type = other_entity_class.upper()                \n",
    "            triples.append(((ngram, entity_class), \"HAS_%s\" % relation_type, (other_ngram, other_entity_class)))\n",
    "        \n",
    "for triple in triples:\n",
    "    print(triple)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Create the graph\n",
    "\n",
    "Now that we have our nodes and relations we can go ahead and build the Neo4J graph.\n",
    "\n",
    "To do this we are going to use py2neo, a Python library for interacting with Neo4J.\n",
    "\n",
    "There are also a couple of other ways to do this - you can either use Neo4J and run Cypher queries to insert each node and relation, or use the APOC library to import a list of nodes from a CSV file. Python is the simplest way in my opinion, though.\n",
    "\n",
    "#### Before proceeding, make sure you have created a new graph in Neo4J and that your new Neo4J graph is running.\n",
    "\n",
    "#### If you need to build your graph again, make sure to run this cell before running subsequent cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from py2neo import Graph\n",
    "from py2neo.data import Node, Relationship\n",
    "\n",
    "GRAPH_PASSWORD = \"password\" # Set this to the password of your Neo4J graph\n",
    "\n",
    "graph = Graph(password = GRAPH_PASSWORD)\n",
    "\n",
    "# We will start by deleting all nodes and edges in the current graph.\n",
    "# If we don't do this, we will end up with duplicate nodes and edges when running this script again.\n",
    "graph.delete_all() \n",
    "\n",
    "tx = graph.begin()\n",
    "\n",
    "# We will keep a dictionary of nodes that we have created so far.\n",
    "# This serves two purposes:\n",
    "#  - prevents duplicate nodes\n",
    "#  - provides us with a way to create edges between the nodes\n",
    "created_entity_nodes = {}\n",
    "\n",
    "# Creates a node for the specified ngram and entity_class.\n",
    "# If the node has already been created (i.e. it exists in created_nodes), return the node.\n",
    "# Otherwise, create a new one.\n",
    "def create_entity_node(ngram, entity_class):\n",
    "    if ngram in created_entity_nodes:\n",
    "        node = created_entity_nodes[ngram]\n",
    "    else:\n",
    "        node = Node(\"Entity\", entity_class, name=ngram)\n",
    "        created_entity_nodes[ngram] = node\n",
    "        tx.create(node)\n",
    "    return node\n",
    "\n",
    "# Creates a relationship between two nodes.\n",
    "# This does not check for duplicate relationships unlike create_node,\n",
    "# so this code will need to be adjusted on larger datasets.\n",
    "def create_relationship(node_1, node_2, relation):    \n",
    "    relationship = Relationship( node_1, relation, node_2 )\n",
    "    tx.create(relationship)\n",
    "    \n",
    "\n",
    "# Create a node for each triple in the list of triples.\n",
    "# Set the class of each node to the entity_class (e.g. \"activity\", \"item\" or \"observation\").\n",
    "# Create a relationship between the nodes in the triple.\n",
    "for ((ngram_1, entity_class_1), relation, (ngram_2, entity_class_2)) in triples:\n",
    "    \n",
    "    node_1 = create_entity_node(ngram_1, entity_class_1)\n",
    "    node_2 = create_entity_node(ngram_2, entity_class_2)   \n",
    "    \n",
    "    create_relationship(node_1, node_2, relation)\n",
    "    \n",
    "    \n",
    "tx.commit()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Create nodes for the work orders\n",
    "\n",
    "In order to query our graph, we need to create nodes for each work order in our dataset as well. We then need to link each Document node to every Entity node appearing in that document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dateutil.parser import parse as parse_date\n",
    "\n",
    "# Our work_order_data and normalised_work_order entities allow us to do this quite easily,\n",
    "\n",
    "tx = graph.begin()\n",
    "\n",
    "created_work_order_nodes = {}\n",
    "\n",
    "# Dates are a little awkward in Neo4j - we have to convert it to an integer representation in Python.\n",
    "# The APOC library has functions to handle this better.\n",
    "def date_to_int(date):\n",
    "    parsed_date = parse_date(str(date))\n",
    "    date = int(\"%s%s%s\" % (parsed_date.year, str(parsed_date.month).zfill(2), str(parsed_date.day).zfill(2)))\n",
    "    return date\n",
    "\n",
    "# The process of creating a work order node is a bit different to creating an entity,\n",
    "# as we also want to incorporate some of the structured fields onto the node\n",
    "\n",
    "def create_structured_node(index, row, node_type, created_nodes):\n",
    "    if index in created_nodes:\n",
    "        return created_nodes[index]\n",
    "\n",
    "    if 'StartDate' in row:\n",
    "        row['StartDate'] = date_to_int(row['StartDate'])\n",
    "    if 'EndDate' in row:\n",
    "        row['EndDate'] = date_to_int(row['EndDate'])  \n",
    "\n",
    "    node = Node(node_type, **row)\n",
    "    created_nodes[index] = node\n",
    "    tx.create(node)\n",
    "    return node\n",
    "\n",
    "for i, row in enumerate(work_order_data):\n",
    "    node = create_structured_node(i, row, \"WorkOrder\", created_work_order_nodes)\n",
    "    \n",
    "tx.commit()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Link the entities to their corresponding work order nodes\n",
    "\n",
    "In order to properly query our graph, we need to link every Item node to the work order node in which it appears.\n",
    "\n",
    "This allows us to run queries such as \"pumps with electrical issues in the last 3 months\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'graph'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-61-e867688fa6d9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[0mnode_2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreated_work_order_nodes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m         \u001b[0mcreate_relationship\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0mnode_1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"APPEARS_IN\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnode_2\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-59-efbefac1d269>\u001b[0m in \u001b[0;36mcreate_relationship\u001b[1;34m(node_1, node_2, relation)\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mcreate_relationship\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode_1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnode_2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrelation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m     \u001b[0mrelationship\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRelationship\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0mnode_1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrelation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnode_2\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 37\u001b[1;33m     \u001b[0mtx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrelationship\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     38\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\micha\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\py2neo\\database.py\u001b[0m in \u001b[0;36mcreate\u001b[1;34m(self, subgraph)\u001b[0m\n\u001b[0;32m    904\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"No method defined to create object %r\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0msubgraph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    905\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 906\u001b[1;33m             \u001b[0mcreate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    907\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    908\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdelete\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msubgraph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\micha\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\py2neo\\data.py\u001b[0m in \u001b[0;36m__db_create__\u001b[1;34m(self, tx)\u001b[0m\n\u001b[0;32m    607\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    608\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__db_create__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 609\u001b[1;33m         \u001b[0mcreate_subgraph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    610\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    611\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__db_delete__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\micha\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\py2neo\\internal\\operations.py\u001b[0m in \u001b[0;36mcreate_subgraph\u001b[1;34m(tx, subgraph)\u001b[0m\n\u001b[0;32m    132\u001b[0m     \"\"\"\n\u001b[0;32m    133\u001b[0m     \u001b[0mgraph\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 134\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnodes\u001b[0m \u001b[1;32min\u001b[0m \u001b[0m_node_create_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mn\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msubgraph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnodes\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraph\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    135\u001b[0m         \u001b[0midentities\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_create_nodes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnodes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    136\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0midentity\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0midentities\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\micha\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\py2neo\\internal\\operations.py\u001b[0m in \u001b[0;36m_node_create_dict\u001b[1;34m(nodes)\u001b[0m\n\u001b[0;32m     42\u001b[0m     \"\"\"\n\u001b[0;32m     43\u001b[0m     \u001b[0md\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mnode\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnodes\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m         \u001b[0mkey\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfrozenset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m         \u001b[0md\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msetdefault\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\micha\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\py2neo\\internal\\operations.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    132\u001b[0m     \"\"\"\n\u001b[0;32m    133\u001b[0m     \u001b[0mgraph\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 134\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnodes\u001b[0m \u001b[1;32min\u001b[0m \u001b[0m_node_create_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mn\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msubgraph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnodes\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraph\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    135\u001b[0m         \u001b[0midentities\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_create_nodes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnodes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    136\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0midentity\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0midentities\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'str' object has no attribute 'graph'"
     ]
    }
   ],
   "source": [
    "tx = graph.begin()\n",
    "\n",
    "# We can use the normalised_work_order_entries list to do this.\n",
    "for i, row in enumerate(normalised_work_order_entities):\n",
    "    for (ngram, entity_class) in row:        \n",
    "        if entity_class != \"item\": continue\n",
    "        \n",
    "        node_1 = created_entity_nodes[ngram]\n",
    "        node_2 = created_work_order_nodes[i]\n",
    "        \n",
    "        relationship = Relationship( node_1, \"APPEARS_IN\", node_2 )\n",
    "        tx.create(relationship)\n",
    "       \n",
    "tx.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Extending the graph to also include Downtime events\n",
    "\n",
    "The next step is to incorporate the downtime events.\n",
    "\n",
    "For this exercise we are going to link the Downtime events to the first Item node appearing in the work orders with the same FLOC as the downtime event.\n",
    "\n",
    "\n",
    "![alt text](images/adding-downtime-events.png \"Adding downtime events\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "tx = graph.begin()\n",
    "\n",
    "created_downtime_nodes = {}\n",
    "\n",
    "\n",
    "# Create a DowntimeEvent node for each row\n",
    "for i, downtime_row in enumerate(downtime_data):\n",
    "    node = create_structured_node(i, downtime_row, \"DowntimeEvent\", created_downtime_nodes)\n",
    "    \n",
    "    # Get all work order nodes with the same FLOC and link the DowntimeEvent to the Items appearing\n",
    "    # in those work orders\n",
    "    for j, work_order_row in enumerate(work_order_data):\n",
    "        if work_order_row[\"FLOC\"] == downtime_row[\"FLOC\"]:\n",
    "            \n",
    "            work_order_entities = normalised_work_order_entities[j]\n",
    "            \n",
    "            for (ngram, entity_class) in work_order_entities:\n",
    "                if entity_class != \"item\": continue                   \n",
    "                    \n",
    "                item_node = created_entity_nodes[ngram]\n",
    "                relationship = Relationship( item_node, \"HAS_EVENT\", node )\n",
    "                tx.create(relationship)\n",
    "                break\n",
    "\n",
    "    \n",
    "tx.commit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Future improvements\n",
    "\n",
    "Our downtime events are currently linked to Item nodes, but it would make more sense to link them to nodes representing the functional locations. \n",
    "\n",
    "If you are interested in continuing work on this small graph, the next best step would be to create nodes for the functional location data (`floc_data`) and to link the downtime events to those nodes as opposed to the Item nodes.\n",
    "\n",
    "![alt text](images/adding-flocs.png \"Adding FLOCs\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
